# Azure AI Content Safety
User-generated content is being posted at an unprecendented scale.

The need for improving online content safety has four main catalysts:
- An increase in harmful content
- Regulatory pressure
- Transparency
- Complex (multimodal) content

Azure AI Content Safety is an Azure AI resource for detecting and managing harmful content in both user-generated and AI-generated material. It also replaces the deprecated Azure Content Moderator.

## What is AI Content Safety?
AI Content Safety is an Azure resource with a set of content moderating capabilities that can be integrated into your applications.

### Trusting User-Generated Content
As social interaction becomes more prevalent in digital space, genuine user-generated content is seen as independent and trustworthy, and can be used as a method of marketing.

Harmful user-generated content has a damaging effect on individuals and brands that could have benefitted from positive user content.

### Content Safety in Azure AI Foundry
Azure AI Content Safety is available in [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-studio/what-is-ai-studio), which is a platform for exploring AI capabilities and developing AI-focused applications.

Azure AI Content Safety Studio allows you to test content safety features for yourself, as well as generating sample code in C#, Java, or Python.

## How Does Azure AI Content Safety Work?
AI Content Safety works with text, images, and AI-generated content.

Content Safety's vision capabilities are powered by Microsoft's Florence model. Text analysis uses NLP techniques, resulting in better understanding of nuance and context. AI Content Safety can detect harmful content in both short form and long form, and is currently available in these languages:
- English
- German
- Spanish
- French
- Portuguese
- Italian
- Chinese

A severity level for each of these categories is used to determine whether the content should be blocked, sent to a moderator, or auto approved:
- Hate
- Sexual
- Self-harm
- Violence

### Safeguarding Text Content
- Moderate text
    - Scans text across the four aforementioned categories
        - A severity level of 0 to 6 is returned for each category
    - You can create a blocklist to scan for specific terms
- Prompt shields
    - Unified API to block jailbreak attacks, which are attempts to bypass LLM safety features
        - Includes user input and documents
- Protected material detection
    - Checks AI-generated text for protected information such as recipes, copyrighted lyrics, etc.
- Groundedness detection
    - Protects against inaccurate AI-generated responses in text
    - A grounded response is one where the model's output is based on the source information
    - An ungrounded response is one where the model's output varies from the source information
    - Groundedness detection includes a reasoning option in the API response, which adds a reasoning field that explains any ungroundedness detection.
        - Reasoning increase cost and processing time.

### Safeguarding Image Content
- Moderate images
    - Scans for inappropriate content across the four categories
        - Severity levels are either safe, low, or high
        - You also set threshold levels of low, medium or high
        - The combination of severity and threshold determines whether the image is allowed or blocked for each category
- Moderate multimodal content
    - Scans both images and text
        - This includes text extracted from an image using optical character recognition (OCR)
    - Content is analyzed across the four categories

### Custom Safety Solutions
- Custom categories
    - Create your own categories by providing positive and negative examples, as well as training the model
        - Content can be scanned according to your own cateogory definitions
    - Safety system messages
        - Assists in writing effective prompts to guide an AI system's behavior

### Limitations
AI Content Safety itself relies on AI, so it may not always catch everything it should. It could also block acceptable language.

This means it should be tested and evaluated on real data before being deployed in addition to continuously monitoring the software's performance post-deployment.

### Evaluating Accuracy
Azure AI Content Safety works best when supporting human moderators who can resolve cases of incorrect identification. Communicate with your users about why content is removed or flagged so everyone understands what is permissible and what is not.

Confusion matrices can help in evaluating the accuracy of AI Content Safety.

## Summary
As the amount of user-generated content continues to rapdily increase, it becomes more important for moderators to effectively control harmful content.

Azure AI Content Safety uses AI to automatically detect violent, sexual, self-harm, or hateful language in real time. It assigns a severity level to the harmful content so that human moderators can focus on high-priority cases, as well as reduce their exposure to disturbing content.

## Further Reading
### [Lab - Use an Azure AI Services Container](https://microsoftlearning.github.io/mslearn-ai-services/Instructions/Exercises/04-use-a-container.html)
### [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-studio/what-is-ai-studio)
### [Jailbreaking LLMs](https://unit42.paloaltonetworks.com/jailbreak-llms-through-camouflage-distraction/#:~:text=Jailbreaking%20refers%20to%20attempts%20to,or%20assist%20in%20harmful%20activities.)