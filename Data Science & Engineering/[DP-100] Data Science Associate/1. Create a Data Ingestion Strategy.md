# Create a Data Ingestion Strategy

You can follow these steps to plan, train, deploy, and monitor a model:
1. Define the problem
    - What should the model predict?
2. Get the data
    - Find sources and get access
3. Prepare the data
    - Examine the data
    - Clean and transform the data based on your needs
4. Train the model
    - Select an algorithm and hyperparameter values based on trial and error
5. Integrate
    - Deploy the model to an endpoint to generate predictions
6. Monitor
    - Track the model's performance.

When gathering data on which to train your model, it's generally best to follow the ETL process:
- Extract
    - Extract the data from its source
- Transform
    - Transform the data to prevent errors from occurring
- Load
    - Load the data into a serving layer

Before the ETL process begins, you need to identify your data source and format.

## Identify the Data Source

The data you want to use could already be stored in a database or be generated by an application.

This means that you may already have a process of generating/acquiring data and storing it. However you could still create an alternate process, acquire publicly available data, or purchase a dataset.

## Identify the Data Format

The format in which you received your data could be different from the format required by your machine learning workload. 

There are three common formats you'll encounter:

1. Tabular (structured)
    - All data has the same properties which are defined in a schema.
    - Columns represent features and rows are the data points.
    - Examples: Excel or CSV

2. Semi-structured
    - Not all data has the same properties.
    - Data is represented as key-value pairs.
    - Examples: IoT devices usually generate JSON data.

3. Unstructured
    - Files do not adhere to any particular structure.
    - Storing unstructured data means no schema needs to be defined, but you also cannot query that data.
    - Examples: images, videos, audio files.

## Identify the Desired Data Format

You may be in a situation where your desired data format requires transforming the data.

For example, if you're using an IoT device returing JSON data tracking the temperature of a machine every minute, you may want to average all the temperatures during a period of time.

This may be better suited to a table, which requires transforming the semi-structured data into structured tabular data. 

After defining your desired format, you can think about how to serve the data.

## Serving Data to ML Workflows

Storing data separate from your compute resources is a good practice. It ensures that you can still access your data during times when computing isn't necessary, thereby reducing costs.

Training machine learning models in Azure typically uses Azure Machine Learning, Azure Databricks, or Azure Synapse Analytics. These services are easily connected to these Azure storage solutions:

- Azure Blob Storage
    - Used for unstructured data or CSV files
- Azure Data Lake Storage (Gen 2)
    - The same as Blob Storage but it uses a hierarchical namespace
- Azure SQL Database
    - Stores structured data

Once you've decided on a method for serving the data to an Azure ML solution, you can design an ETL pipeline.

## Design a Data Ingestion Pipeline

Data ingestion pipelines can be created using any of these Azure services:

- Azure Synapse Analytics
    - Azure Synapse Pipelines allows the creation and scheduling of data ingestion pipelines with a GUI or a JSON document.
    - There are many standard connectors available within the service to easily copy data from the source to your data store.
    - Adding data transformation tasks to your pipeline can be done with a UI tool like mapping data flow or with SQL, Python, or R.
    - You can also choose different types of compute for handling large data transformations at scale:
        - Serverless SQL pools
        - Dedicated SQL pools
        - Spark pools
- Azure Databricks
    - Code-first pipeline creation with SQL, Python, and R
        - Define pipelines in a notebook that can be scheduled to run
    - Uses Spark clusters which distribute the compute to transform large amounts of data quickly
    - Can also train models
- Azure Machine Learning
    - Compute clusters that scale up and down when needed
    - Pipelines can be created with Designer or with scripts.
    - The service is used primarily for training ML models, but it has ETL functionality too.

Each of these services are able to perform ETL work, but there are some pros and cons to each.

Once you have determined which of these services you will use, you can put together your pipeline.

An example pipeline may look like this:

1. Extract raw data from its source.
2. Copy and transform the data.
    - Azure Synapse Analytics
3. Store the prepared data.
    - Azure Blob Storage
4. Train the model.
    - Azure Machine Learning

Automating this process is very important to consider as you work on your model.

## Quiz

https://microsoftlearning.github.io/mslearn-aml-design/Instructions/start-01-data.html